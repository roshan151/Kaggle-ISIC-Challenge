{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYe1M3suIkST",
        "outputId": "18fb1ce1-4ab9-4e50-e979-b5af79ad27ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n",
            "Collecting tensorflow-probability==0.23.0\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.26.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.1.8)\n",
            "Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-probability\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.24.0\n",
            "    Uninstalling tensorflow-probability-0.24.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.24.0\n",
            "Successfully installed tensorflow-probability-0.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow-probability==0.23.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl11tm-GkYvF",
        "outputId": "43981d70-3370-4a09-8df7-0f3e3e42b9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive in colab\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "# Unzip dataset file in colab\n",
        "kaggle_dir = '/content/gdrive/MyDrive/Kaggle/'\n",
        "\n",
        "filename = 'isic-2024-challenge.zip'\n",
        "\n",
        "with zipfile.ZipFile(f'{kaggle_dir}/{filename}', 'r') as zp:\n",
        "  zp.extractall('Kaggle-ISIC-Challenge/dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0DWAxk6oids",
        "outputId": "e2804b1d-4a74-4654-b7f9-c71b2a290500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 18 03:18:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              45W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzznUp4GEgm0",
        "outputId": "abbe661f-c8ad-4cea-f07c-b59f7f5b256c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a3300a721e99>:2: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_df = pd.read_csv(train_csv)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: Index(['isic_id', 'target', 'patient_id', 'age_approx', 'sex',\n",
            "       'anatom_site_general', 'clin_size_long_diam_mm', 'image_type',\n",
            "       'tbp_tile_type', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext',\n",
            "       'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L',\n",
            "       'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio',\n",
            "       'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB',\n",
            "       'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm',\n",
            "       'tbp_lv_eccentricity', 'tbp_lv_location', 'tbp_lv_location_simple',\n",
            "       'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border',\n",
            "       'tbp_lv_norm_color', 'tbp_lv_perimeterMM',\n",
            "       'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt',\n",
            "       'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y',\n",
            "       'tbp_lv_z', 'attribution', 'copyright_license', 'lesion_id',\n",
            "       'iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5',\n",
            "       'mel_mitotic_index', 'mel_thick_mm', 'tbp_lv_dnn_lesion_confidence'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a3300a721e99>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  continous_vars['target'] = train_df['target']\n",
            "<ipython-input-3-a3300a721e99>:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  continous_vars['id'] = train_df['isic_id']\n"
          ]
        }
      ],
      "source": [
        "train_csv = f'Kaggle-ISIC-Challenge/dataset/train-metadata.csv'\n",
        "train_df = pd.read_csv(train_csv)\n",
        "\n",
        "columns = train_df.columns\n",
        "\n",
        "print(f'Columns: {columns}')\n",
        "\n",
        "# TODO: Add all variables\n",
        "# Continous columns- starting analysis on a subset of columns for now\n",
        "continous = ['age_approx',\n",
        " 'tbp_lv_C',\n",
        " 'clin_size_long_diam_mm',\n",
        " 'tbp_lv_A',\n",
        " 'tbp_lv_Aext',\n",
        " 'tbp_lv_B',\n",
        " 'tbp_lv_Bext',\n",
        " 'tbp_lv_C',\n",
        " 'tbp_lv_Cext',\n",
        " 'tbp_lv_H',\n",
        " 'tbp_lv_Hext',\n",
        "'tbp_lv_L',\n",
        "'tbp_lv_Lext',\n",
        "'tbp_lv_areaMM2',\n",
        "'tbp_lv_area_perim_ratio',\n",
        "'tbp_lv_color_std_mean',\n",
        "'tbp_lv_deltaA',\n",
        "'tbp_lv_deltaB',\n",
        "'tbp_lv_deltaL',\n",
        "'tbp_lv_deltaLB',\n",
        "'tbp_lv_deltaLBnorm',\n",
        "'tbp_lv_eccentricity',]\n",
        "\n",
        "# Categorical columns\n",
        "categorical = ['anatom_site_general',\n",
        " 'tbp_tile_type',\n",
        " 'image_type',\n",
        " 'tbp_lv_location',\n",
        " 'tbp_lv_location_simple']\n",
        "\n",
        "continous_vars = train_df[continous]\n",
        "categorical_vars = train_df[categorical]\n",
        "\n",
        "continous_vars['target'] = train_df['target']\n",
        "continous_vars['id'] = train_df['isic_id']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_YQ-FUQD9EF",
        "outputId": "b4fa8a0b-6656-4d9d-e509-22238bd426f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (320847, 24)\n",
            "Test shape: (80212, 24)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df = continous_vars.copy()\n",
        "# Normalize columns\n",
        "for col in continous:\n",
        "  train_df[col] = (train_df[col] - train_df[col].min()) / (train_df[col].max() - train_df[col].min())\n",
        "\n",
        "# Get NaN value counts age_approx : 2798\n",
        "#nan_counts = continous_vars.isna().sum()\n",
        "mean_age = train_df['age_approx'].mean()\n",
        "train_df['age_approx'].fillna(mean_age, inplace=True)\n",
        "\n",
        "# Split dataset\n",
        "train, test = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Train shape: {train.shape}')\n",
        "print(f'Test shape: {test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky3HQl26FXdT",
        "outputId": "f5471b07-297e-4a9a-9416-6dfb9ae07078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial: target\n",
            "0    320528\n",
            "1       319\n",
            "Name: count, dtype: int64\n",
            "Final target\n",
            "0    64105\n",
            "1      319\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print('Initial:', train['target'].value_counts())\n",
        "\n",
        "def undersample_data(df, divisor = 5):\n",
        "  # Undersample class 0 by 1/5 times\n",
        "  cls_indices = [idx for idx, cls in enumerate(df['target'].values) if cls == 0]\n",
        "  cls_1 = [idx for idx, cls in enumerate(df['target'].values) if cls == 1]\n",
        "  to_keep = len(cls_indices)//5\n",
        "  np.random.shuffle(cls_indices)\n",
        "  indices_to_keep = cls_indices[:to_keep]\n",
        "  all_indices = indices_to_keep + cls_1\n",
        "\n",
        "  np.random.shuffle(all_indices)\n",
        "  df = df.iloc[all_indices]\n",
        "  return df\n",
        "\n",
        "train = undersample_data(train)\n",
        "test = undersample_data(test)\n",
        "\n",
        "print('Final', train['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_w8xiIaOFaud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4d6618-98ba-4a91-9ca8-f802857bc647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "image_dir = 'Kaggle-ISIC-Challenge/dataset/train-image/image'\n",
        "IMAGE_SIZE = 224\n",
        "vgg_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights = 'imagenet')\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D2qGt5jKGF4p"
      },
      "outputs": [],
      "source": [
        "# Data Generator\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "  def __init__(self, image_ids, df, labels, batch_size=16):\n",
        "\n",
        "    self.image_ids = image_ids\n",
        "    self.df = df\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.IMAGE_SIZE = 224\n",
        "    self.image_dir = 'Kaggle-ISIC-Challenge/dataset/train-image/image'\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(len(self.df) / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    batch_df = self.df[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "\n",
        "    # Image features\n",
        "    batch_ids = self.image_ids[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "\n",
        "    batch_images = []\n",
        "    image_features = []\n",
        "    for id in batch_ids:\n",
        "      q = Image.open(f'{image_dir}/{id}.jpg')\n",
        "      q = np.array(q.resize((self.IMAGE_SIZE, self.IMAGE_SIZE)))\n",
        "      features = vgg_model(np.array(q).reshape(1, self.IMAGE_SIZE, self.IMAGE_SIZE, 3))\n",
        "      image_features.append(np.array(Flatten()(features)).reshape(7*7*512,))\n",
        "      batch_images.append(q)\n",
        "\n",
        "    length = len(batch_images)\n",
        "\n",
        "    #image_features = tf.reshape(image_features, (length, -1))\n",
        "\n",
        "    numerical_features = batch_df.to_numpy()\n",
        "\n",
        "    # Batch labels\n",
        "    batch_y = self.labels[index*self.batch_size : (index+1)*self.batch_size]\n",
        "\n",
        "    return [np.array(image_features), numerical_features], batch_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkARtIzN1lYc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "NAei1MjGx7P2"
      },
      "outputs": [],
      "source": [
        "dtype= 'float32'\n",
        "train_labels = train['target'].values\n",
        "train_image_ids = train['id'].values\n",
        "train_input = train.drop(['target', 'id'], axis = 1)\n",
        "\n",
        "train_data_gen = DataGenerator(train_image_ids, train_input, train_labels, 1)\n",
        "\n",
        "test_labels = test['target'].values\n",
        "test_image_ids = test['id'].values\n",
        "test_input = test.drop(['target', 'id'], axis = 1)\n",
        "\n",
        "val_data_gen = DataGenerator(test_image_ids, test_input, test_labels,  1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "QMgYYIH0ykUW"
      },
      "outputs": [],
      "source": [
        "x, y = val_data_gen.__getitem__(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAEQymNxj1Ur",
        "outputId": "c3d950ed-c27d-4a5b-c724-059fde857b33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LJrPipIkzkI9"
      },
      "outputs": [],
      "source": [
        "from sys import stdlib_module_names\n",
        "# Setting up a bayesian neural network\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "\n",
        "# A normal distribution (mean = 0, sd = 1) as prior for kernel weights\n",
        "def prior(kernel_size: int, bias_size : int, dtype):\n",
        "  '''\n",
        "  To apply baye's theorem we start with a prior distribution which represents our prior beliefs about the weight distribution\n",
        "  Prior distribution is not trainable\n",
        "  '''\n",
        "  n = kernel_size + bias_size\n",
        "  return lambda t: tfd.Independent(tfd.Normal(loc = tf.zeros(n, dtype = dtype), scale = 1.), reinterpreted_batch_ndims = 1)\n",
        "\n",
        "def random_gaussian_initializer(shape, dtype):\n",
        "  '''\n",
        "  Initialize random normal distributions for out surrogate posterior\n",
        "\n",
        "  shape: number of (means + standard deviations)\n",
        "  '''\n",
        "  n = int(shape/2)\n",
        "  loc_norm = tf.random_normal_initializer(mean = 0., stddev = 1.)\n",
        "  loc = tf.Variable(initial_value = loc_norm(shape = (n,), dtype=dtype))\n",
        "\n",
        "  scale_norm = tf.random_normal_initializer(mean = 2., stddev = 1.)\n",
        "  scale = tf.Variable(initial_value = scale_norm(shape = (n,), dtype=dtype))\n",
        "\n",
        "  return tf.concat([loc, scale], 0)\n",
        "\n",
        "\n",
        "def posterior_mean_field(kernel_size: int, bias_size: int, dtype):\n",
        "  '''\n",
        "  Posterior distribution is a keras sequential model of two layers\n",
        "  1. Layer 1 is made of 2*n trainable variables (representing mean and standard deviation for every kernel)\n",
        "  2. Layer 2 is a tf probability distribution (normal distribution in this case) which is initalized using variables\n",
        "  from the first layer\n",
        "\n",
        "  This distribution represent our final beliefs for the weights of neural network\n",
        "  '''\n",
        "  n = kernel_size + bias_size\n",
        "  c = np.log(np.expm1(1.))\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tfp.layers.VariableLayer(2 * n, dtype = dtype,\n",
        "                    initializer = lambda shape, dtype: random_gaussian_initializer(shape, dtype), trainable = True),\n",
        "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "          tfd.Normal(loc = t[..., :n], scale = 1e-5 + tf.nn.softplus(c + t[..., n:])), reinterpreted_batch_ndims = 1))\n",
        "      ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5YmHRKDixwn",
        "outputId": "4700fc4e-45d1-4499-aa8f-005921da5994"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 25088)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRhYuFYxj84z",
        "outputId": "0ddc5d02-6309-4ce0-b9a7-cfba46d5fc8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64424"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.shape[0]\n",
        "examples = int(train.shape[0])\n",
        "\n",
        "kl_divergence = (lambda q, p, _: tfd.kl_divergence(q, p) / tf.cast(tf.shape(q.sample())[0], tf.float32))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUFdNq0MmABs",
        "outputId": "0793d350-8f28-4db5-edd5-a6de19fe8b75"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(y)#.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MimkCOTDlhBG",
        "outputId": "a2c46735-caea-4cae-df7b-76654aee658f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qm3S6ij45AY",
        "outputId": "e76db14f-dc20-48cd-d5e0-6f03f3f0ddd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-139-894c8e518efa>:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  n = int(shape/2)\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "# Neural Network\n",
        "#image_model_in = Input(x[0].shape)\n",
        "#image_model = tfpl.DenseVariational(units = 1572, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(image_model_in)\n",
        "#image_model = tfpl.DenseVariational(units = 512, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(image_model)\n",
        "#image_model = tfpl.DenseVariational(units = 128, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(image_model)\n",
        "\n",
        "numerical_model_in = Input(x[1].shape)\n",
        "numerical_model = tfpl.DenseVariational(units = 128, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(numerical_model_in)\n",
        "numerical_model = tfpl.DenseVariational(units = 64, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(numerical_model)\n",
        "\n",
        "concat = concatenate([image_model, numerical_model])\n",
        "ensemble_model_in = tfpl.DenseVariational(units = 128, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(concat)\n",
        "ensemble_model_in = tfpl.DenseVariational(units = 32, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(ensemble_model_in)\n",
        "ensemble_model_out = tfpl.DenseVariational(units = 1, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'sigmoid')(ensemble_model_in)\n",
        "ensemble_model_out = tfpl.DistributionLambda(lambda t: tfd.Bernoulli(logits = t))(ensemble_model_out)\n",
        "\n",
        "ensemble_model = Model(inputs = [image_model_in, numerical_model_in], outputs = ensemble_model_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8j_4bu9kGGY",
        "outputId": "baada5a4-fc1a-416b-abc1-87395a67fbfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omv2OgJl6qlC",
        "outputId": "9f5d02f6-73ad-4cc9-9771-4c23d28b305f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 277s 1s/step - loss: 1869.7915 - binary_accuracy: 0.3720 - val_loss: 1847.7811 - val_binary_accuracy: 0.3656\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 272s 1s/step - loss: 1826.3060 - binary_accuracy: 0.3690 - val_loss: 1804.9419 - val_binary_accuracy: 0.3718\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 259s 1s/step - loss: 1784.0408 - binary_accuracy: 0.3715 - val_loss: 1763.0695 - val_binary_accuracy: 0.3738\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 252s 1s/step - loss: 1742.5878 - binary_accuracy: 0.3490 - val_loss: 1722.0516 - val_binary_accuracy: 0.3716\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 252s 1s/step - loss: 1701.9110 - binary_accuracy: 0.3880 - val_loss: 1681.9110 - val_binary_accuracy: 0.3771\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 251s 1s/step - loss: 1662.2520 - binary_accuracy: 0.3735 - val_loss: 1642.6007 - val_binary_accuracy: 0.3743\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 252s 1s/step - loss: 1623.2701 - binary_accuracy: 0.3695 - val_loss: 1603.9955 - val_binary_accuracy: 0.3655\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 251s 1s/step - loss: 1585.1250 - binary_accuracy: 0.3480 - val_loss: 1566.1949 - val_binary_accuracy: 0.3773\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 253s 1s/step - loss: 1547.6691 - binary_accuracy: 0.3600 - val_loss: 1529.1169 - val_binary_accuracy: 0.3699\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 251s 1s/step - loss: 1510.9525 - binary_accuracy: 0.3535 - val_loss: 1492.7780 - val_binary_accuracy: 0.3756\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 251s 1s/step - loss: 1474.9521 - binary_accuracy: 0.3905 - val_loss: 1457.1389 - val_binary_accuracy: 0.3679\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 249s 999ms/step - loss: 1439.6704 - binary_accuracy: 0.3690 - val_loss: 1422.1545 - val_binary_accuracy: 0.3759\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 252s 1s/step - loss: 1405.0358 - binary_accuracy: 0.3730 - val_loss: 1387.8330 - val_binary_accuracy: 0.3701\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 251s 1s/step - loss: 1370.9916 - binary_accuracy: 0.3715 - val_loss: 1354.1675 - val_binary_accuracy: 0.3767\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 253s 1s/step - loss: 1337.6688 - binary_accuracy: 0.3780 - val_loss: 1321.1597 - val_binary_accuracy: 0.3702\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 253s 1s/step - loss: 1304.9800 - binary_accuracy: 0.3640 - val_loss: 1288.7620 - val_binary_accuracy: 0.3725\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 252s 1s/step - loss: 1272.8906 - binary_accuracy: 0.3750 - val_loss: 1256.9729 - val_binary_accuracy: 0.3687\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 252s 1s/step - loss: 1241.4065 - binary_accuracy: 0.3865 - val_loss: 1225.8191 - val_binary_accuracy: 0.3656\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 252s 1s/step - loss: 1210.4695 - binary_accuracy: 0.3620 - val_loss: 1195.2418 - val_binary_accuracy: 0.3767\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 252s 1s/step - loss: 1180.2279 - binary_accuracy: 0.3625 - val_loss: 1165.2494 - val_binary_accuracy: 0.3746\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 251s 1s/step - loss: 1150.5548 - binary_accuracy: 0.3625 - val_loss: 1135.8285 - val_binary_accuracy: 0.3745\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 1121.3956 - binary_accuracy: 0.3725"
          ]
        }
      ],
      "source": [
        "def nll_loss(y_true, y_pred):\n",
        "  nll = -y_pred.log_prob(y_true)\n",
        "  return nll\n",
        "\n",
        "ensemble_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002), loss = nll_loss, metrics = ['binary_accuracy'])\n",
        "ensemble_model.fit(train_data_gen, validation_data = val_data_gen, epochs = 50, steps_per_epoch = 250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnKX1lIHlbWj"
      },
      "outputs": [],
      "source": [
        "# Save model weights\n",
        "ensemble_model.save_weights('/content/gdrive/MyDrive/Kaggle/ensemble_model_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwKtnfy23TB_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}