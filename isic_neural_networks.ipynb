{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYe1M3suIkST",
        "outputId": "3724126e-0692-483d-d579-27098b62012f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-probability==0.23.0 in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.26.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.1.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow-probability==0.23.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Wl11tm-GkYvF",
        "outputId": "645339ea-bbe3-4002-faf2-0a6134c42204"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Mount google drive in colab\\ndrive.mount('/content/gdrive/', force_remount=True)\\n\\n# Unzip dataset file in colab\\nkaggle_dir = '/content/gdrive/MyDrive/Kaggle/'\\n\\nfilename = 'isic-2024-challenge.zip'\\n\\nwith zipfile.ZipFile(f'{kaggle_dir}/{filename}', 'r') as zp:\\n  zp.extractall('Kaggle-ISIC-Challenge/dataset/')\\n  \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt\n",
        "\n",
        "from google.colab import drive\n",
        "'''\n",
        "# Mount google drive in colab\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "# Unzip dataset file in colab\n",
        "kaggle_dir = '/content/gdrive/MyDrive/Kaggle/'\n",
        "\n",
        "filename = 'isic-2024-challenge.zip'\n",
        "\n",
        "with zipfile.ZipFile(f'{kaggle_dir}/{filename}', 'r') as zp:\n",
        "  zp.extractall('Kaggle-ISIC-Challenge/dataset/')\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0DWAxk6oids",
        "outputId": "42763556-9789-4150-8d33-ca196982dd44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 12 17:15:10 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              42W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "lzznUp4GEgm0",
        "outputId": "3a92876e-fdf5-4b3d-d92b-857d237cba8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrain_csv = f'Kaggle-ISIC-Challenge/dataset/train-metadata.csv'\\ntrain_df = pd.read_csv(train_csv)\\n\\ncolumns = train_df.columns\\n\\nprint(f'Columns: {columns}')\\n\\n# TODO: Add all variables\\n# Continous columns- starting analysis on a subset of columns for now\\ncontinous = ['age_approx',\\n 'tbp_lv_C',\\n 'clin_size_long_diam_mm',\\n 'tbp_lv_A',\\n 'tbp_lv_Aext',\\n 'tbp_lv_B',\\n 'tbp_lv_Bext',\\n 'tbp_lv_C',\\n 'tbp_lv_Cext',\\n 'tbp_lv_H',\\n 'tbp_lv_Hext',\\n'tbp_lv_L',\\n'tbp_lv_Lext',\\n'tbp_lv_areaMM2',\\n'tbp_lv_area_perim_ratio',\\n'tbp_lv_color_std_mean',\\n'tbp_lv_deltaA',\\n'tbp_lv_deltaB',\\n'tbp_lv_deltaL',\\n'tbp_lv_deltaLB',\\n'tbp_lv_deltaLBnorm',\\n'tbp_lv_eccentricity',]\\n\\n# Categorical columns\\ncategorical = ['anatom_site_general',\\n 'tbp_tile_type',\\n 'image_type',\\n 'tbp_lv_location',\\n 'tbp_lv_location_simple']\\n\\ncontinous_vars = train_df[continous]\\ncategorical_vars = train_df[categorical]\\n\\ncontinous_vars['target'] = train_df['target']\\ncontinous_vars['id'] = train_df['isic_id']\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "train_csv = f'Kaggle-ISIC-Challenge/dataset/train-metadata.csv'\n",
        "train_df = pd.read_csv(train_csv)\n",
        "\n",
        "columns = train_df.columns\n",
        "\n",
        "print(f'Columns: {columns}')\n",
        "\n",
        "# TODO: Add all variables\n",
        "# Continous columns- starting analysis on a subset of columns for now\n",
        "continous = ['age_approx',\n",
        " 'tbp_lv_C',\n",
        " 'clin_size_long_diam_mm',\n",
        " 'tbp_lv_A',\n",
        " 'tbp_lv_Aext',\n",
        " 'tbp_lv_B',\n",
        " 'tbp_lv_Bext',\n",
        " 'tbp_lv_C',\n",
        " 'tbp_lv_Cext',\n",
        " 'tbp_lv_H',\n",
        " 'tbp_lv_Hext',\n",
        "'tbp_lv_L',\n",
        "'tbp_lv_Lext',\n",
        "'tbp_lv_areaMM2',\n",
        "'tbp_lv_area_perim_ratio',\n",
        "'tbp_lv_color_std_mean',\n",
        "'tbp_lv_deltaA',\n",
        "'tbp_lv_deltaB',\n",
        "'tbp_lv_deltaL',\n",
        "'tbp_lv_deltaLB',\n",
        "'tbp_lv_deltaLBnorm',\n",
        "'tbp_lv_eccentricity',]\n",
        "\n",
        "# Categorical columns\n",
        "categorical = ['anatom_site_general',\n",
        " 'tbp_tile_type',\n",
        " 'image_type',\n",
        " 'tbp_lv_location',\n",
        " 'tbp_lv_location_simple']\n",
        "\n",
        "continous_vars = train_df[continous]\n",
        "categorical_vars = train_df[categorical]\n",
        "\n",
        "continous_vars['target'] = train_df['target']\n",
        "continous_vars['id'] = train_df['isic_id']\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'Kaggle-ISIC-Challenge/dataset'\n",
        "df_train = pd.read_csv(f\"{directory}/train-metadata.csv\")\n",
        "df_test = pd.read_csv(f\"{directory}/test-metadata.csv\")\n",
        "\n",
        "#train_target = df_train['target']\n",
        "#test_target = df_test['target']\n",
        "\n",
        "def feature_engineering(df):\n",
        "    # New features to try...\n",
        "    df[\"lesion_size_ratio\"] = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n",
        "    df[\"lesion_shape_index\"] = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n",
        "    df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n",
        "    df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n",
        "    df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n",
        "    df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n",
        "    df[\"color_uniformity\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n",
        "    df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2)\n",
        "    df[\"perimeter_to_area_ratio\"] = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n",
        "    df[\"area_to_perimeter_ratio\"] = df[\"tbp_lv_areaMM2\"] / df[\"tbp_lv_perimeterMM\"]\n",
        "    df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n",
        "    df[\"combined_anatomical_site\"] = df[\"anatom_site_general\"] + \"_\" + df[\"tbp_lv_location\"]\n",
        "    df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n",
        "    df[\"symmetry_border_consistency2\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"] / (df[\"tbp_lv_symm_2axis\"] + df[\"tbp_lv_norm_border\"])\n",
        "    df[\"color_consistency\"] = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n",
        "    df[\"color_consistency2\"] = df[\"tbp_lv_stdL\"] * df[\"tbp_lv_Lext\"] / (df[\"tbp_lv_stdL\"] + df[\"tbp_lv_Lext\"])\n",
        "\n",
        "    df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n",
        "    df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n",
        "    df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n",
        "    df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n",
        "    df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n",
        "    df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n",
        "    df[\"normalized_lesion_size\"] = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n",
        "    df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n",
        "    df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n",
        "    df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n",
        "    df[\"3d_lesion_orientation\"] = np.arctan2(df_train[\"tbp_lv_y\"], df_train[\"tbp_lv_x\"])\n",
        "    df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n",
        "    df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n",
        "    df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n",
        "\n",
        "    # Taken from: https://www.kaggle.com/code/dschettler8845/isic-detect-skin-cancer-let-s-learn-together\n",
        "    df[\"color_variance_ratio\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_stdLExt\"]\n",
        "    df[\"border_color_interaction\"] = df[\"tbp_lv_norm_border\"] * df[\"tbp_lv_norm_color\"]\n",
        "    df[\"size_color_contrast_ratio\"] = df[\"clin_size_long_diam_mm\"] / df[\"tbp_lv_deltaLBnorm\"]\n",
        "    df[\"age_normalized_nevi_confidence\"] = df[\"tbp_lv_nevi_confidence\"] / df[\"age_approx\"]\n",
        "    df[\"color_asymmetry_index\"] = df[\"tbp_lv_radial_color_std_max\"] * df[\"tbp_lv_symm_2axis\"]\n",
        "    df[\"3d_volume_approximation\"] = df[\"tbp_lv_areaMM2\"] * np.sqrt(df[\"tbp_lv_x\"]**2 + df[\"tbp_lv_y\"]**2 + df[\"tbp_lv_z\"]**2)\n",
        "    df[\"color_range\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs() + (df[\"tbp_lv_A\"] - df[\"tbp_lv_Aext\"]).abs() + (df[\"tbp_lv_B\"] - df[\"tbp_lv_Bext\"]).abs()\n",
        "    df[\"shape_color_consistency\"] = df[\"tbp_lv_eccentricity\"] * df[\"tbp_lv_color_std_mean\"]\n",
        "    df[\"border_length_ratio\"] = df[\"tbp_lv_perimeterMM\"] / (2 * np.pi * np.sqrt(df[\"tbp_lv_areaMM2\"] / np.pi))\n",
        "    df[\"age_size_symmetry_index\"] = df[\"age_approx\"] * df[\"clin_size_long_diam_mm\"] * df[\"tbp_lv_symm_2axis\"]\n",
        "    df[\"age_size_symmetry_index2\"] = df[\"age_approx\"] * df[\"tbp_lv_areaMM2\"] * df[\"tbp_lv_symm_2axis\"]\n",
        "    # Until here.\n",
        "\n",
        "    new_num_cols = [\n",
        "        \"lesion_size_ratio\", \"lesion_shape_index\", \"hue_contrast\",\n",
        "        \"luminance_contrast\", \"lesion_color_difference\", \"border_complexity\",\n",
        "        \"color_uniformity\", \"3d_position_distance\", \"perimeter_to_area_ratio\",\"area_to_perimeter_ratio\",\n",
        "        \"lesion_visibility_score\", \"symmetry_border_consistency\", \"symmetry_border_consistency2\", \"color_consistency\",\"color_consistency2\",\n",
        "\n",
        "        \"size_age_interaction\", \"hue_color_std_interaction\", \"lesion_severity_index\",\n",
        "        \"shape_complexity_index\", \"color_contrast_index\", \"log_lesion_area\",\n",
        "        \"normalized_lesion_size\", \"mean_hue_difference\", \"std_dev_contrast\",\n",
        "        \"color_shape_composite_index\", \"3d_lesion_orientation\", \"overall_color_difference\",\n",
        "        \"symmetry_perimeter_interaction\", \"comprehensive_lesion_index\",\n",
        "\n",
        "        \"color_variance_ratio\", \"border_color_interaction\", \"size_color_contrast_ratio\",\n",
        "        \"age_normalized_nevi_confidence\", \"color_asymmetry_index\", \"3d_volume_approximation\",\n",
        "        \"color_range\", \"shape_color_consistency\", \"border_length_ratio\", \"age_size_symmetry_index\",\"age_size_symmetry_index2\",\n",
        "    ]\n",
        "    new_cat_cols = [\"combined_anatomical_site\"]\n",
        "\n",
        "    #生成zscore特征\n",
        "    floating_df = df.loc[:,df.dtypes == np.floating]\n",
        "    mean = floating_df.mean()\n",
        "    std = floating_df.std()\n",
        "    zscore = (floating_df - mean)/std\n",
        "    zscore = zscore[[x for x in zscore.columns if x in new_num_cols]]\n",
        "    zscore.columns = [f\"zscore_{col}\" for col in zscore.columns]\n",
        "    df = pd.concat([df,zscore],axis = 1)\n",
        "    new_num_cols.extend(zscore.columns)\n",
        "\n",
        "    #floating_df = df.loc[:,df.dtypes == np.floating]\n",
        "    #mean = floating_df.mean()\n",
        "    #std = floating_df.std()\n",
        "    #zscore = (floating_df - mean)/std\n",
        "    #zscore.columns = [f\"zscore_{col}\" for col in zscore.columns]\n",
        "    #try:\n",
        "    #    zscore.drop(['zscore_mel_thick_mm', 'zscore_tbp_lv_dnn_lesion_confidence'],axis = 1,inplace = True)\n",
        "    #except:\n",
        "    #    continue\n",
        "    #df = pd.concat([df,zscore],axis = 1)\n",
        "    #new_num_cols.extend(zscore.columns)\n",
        "\n",
        "    return df, new_num_cols, new_cat_cols\n",
        "\n",
        "num_cols = [\n",
        "    'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext',\n",
        "    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L',\n",
        "    'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean',\n",
        "    'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n",
        "    'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n",
        "    'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n",
        "    'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n",
        "    'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n",
        "    'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z',\n",
        "]\n",
        "\n",
        "df_train[num_cols] = df_train[num_cols].fillna(df_train[num_cols].median())\n",
        "df_test [num_cols] = df_test [num_cols].fillna(df_train[num_cols].median())\n",
        "\n",
        "df_train, new_num_cols, new_cat_cols = feature_engineering(df_train.copy())\n",
        "df_test, _, _                        = feature_engineering(df_test.copy())\n",
        "\n",
        "num_cols += new_num_cols\n",
        "\n",
        "# anatom_site_general\n",
        "cat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"] + new_cat_cols\n",
        "train_cols = num_cols + cat_cols\n",
        "\n",
        "df_train = df_train[train_cols + ['isic_id', 'target']]\n",
        "#df_test = df_test[train_cols + ['isic_id']]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFXflBESuCs3",
        "outputId": "4ccb7c5a-7c1c-4f53-a610-612b9b4b6397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-62-20405c3b5938>:2: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_train = pd.read_csv(f\"{directory}/train-metadata.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_categorical_cols(df, categorical_cols):\n",
        "  \"\"\"Converts specified categorical columns in a DataFrame to one-hot encoding.\n",
        "\n",
        "  Args:\n",
        "    df: The Pandas DataFrame.\n",
        "    categorical_cols: A list of column names to be one-hot encoded.\n",
        "\n",
        "  Returns:\n",
        "    A DataFrame with the specified categorical columns one-hot encoded.\n",
        "  \"\"\"\n",
        "  # Create a copy of the DataFrame to avoid modifying the original\n",
        "  encoded_df = df.copy()\n",
        "\n",
        "  # Apply one-hot encoding to each categorical column\n",
        "  for col in categorical_cols:\n",
        "    # Get one-hot encoded columns for the current categorical column\n",
        "    if col not in encoded_df.columns:\n",
        "      print(f'{col} not found')\n",
        "      continue\n",
        "    one_hot_encoded = pd.get_dummies(encoded_df[col], prefix=col)\n",
        "\n",
        "    # Drop the original categorical column\n",
        "    encoded_df = encoded_df.drop(col, axis=1)\n",
        "\n",
        "    # Concatenate the one-hot encoded columns with the DataFrame\n",
        "    encoded_df = pd.concat([encoded_df, one_hot_encoded], axis=1)\n",
        "\n",
        "  return encoded_df\n",
        "cat_cols_extra = ['sex_female', 'sex_male', 'tbp_tile_type_3D: XP', 'tbp_tile_type_3D: white', 'tbp_lv_location_Head & Neck', 'tbp_lv_location_Left Arm', 'tbp_lv_location_Left Arm - Lower', 'tbp_lv_location_Left Arm - Upper', 'tbp_lv_location_Left Leg', 'tbp_lv_location_Left Leg - Lower', 'tbp_lv_location_Left Leg - Upper', 'tbp_lv_location_Right Arm', 'tbp_lv_location_Right Arm - Lower', 'tbp_lv_location_Right Arm - Upper', 'tbp_lv_location_Right Leg', 'tbp_lv_location_Right Leg - Lower', 'tbp_lv_location_Right Leg - Upper', 'tbp_lv_location_Torso Back', 'tbp_lv_location_Torso Back Bottom Third', 'tbp_lv_location_Torso Back Middle Third', 'tbp_lv_location_Torso Back Top Third', 'tbp_lv_location_Torso Front', 'tbp_lv_location_Torso Front Bottom Half', 'tbp_lv_location_Torso Front Top Half', 'tbp_lv_location_Unknown', 'tbp_lv_location_simple_Head & Neck', 'tbp_lv_location_simple_Left Arm', 'tbp_lv_location_simple_Left Leg', 'tbp_lv_location_simple_Right Arm', 'tbp_lv_location_simple_Right Leg', 'tbp_lv_location_simple_Torso Back', 'tbp_lv_location_simple_Torso Front', 'tbp_lv_location_simple_Unknown', 'combined_anatomical_site_anterior torso_Torso Front', 'combined_anatomical_site_anterior torso_Torso Front Bottom Half', 'combined_anatomical_site_anterior torso_Torso Front Top Half', 'combined_anatomical_site_head/neck_Head & Neck', 'combined_anatomical_site_lower extremity_Left Leg', 'combined_anatomical_site_lower extremity_Left Leg - Lower', 'combined_anatomical_site_lower extremity_Left Leg - Upper', 'combined_anatomical_site_lower extremity_Right Leg', 'combined_anatomical_site_lower extremity_Right Leg - Lower', 'combined_anatomical_site_lower extremity_Right Leg - Upper', 'combined_anatomical_site_posterior torso_Torso Back', 'combined_anatomical_site_posterior torso_Torso Back Bottom Third', 'combined_anatomical_site_posterior torso_Torso Back Middle Third', 'combined_anatomical_site_posterior torso_Torso Back Top Third', 'combined_anatomical_site_upper extremity_Left Arm', 'combined_anatomical_site_upper extremity_Left Arm - Lower', 'combined_anatomical_site_upper extremity_Left Arm - Upper', 'combined_anatomical_site_upper extremity_Right Arm', 'combined_anatomical_site_upper extremity_Right Arm - Lower', 'combined_anatomical_site_upper extremity_Right Arm - Upper']\n",
        "#df_test = one_hot_encode_categorical_cols(df_test, cat_cols+cat_cols_extra)\n",
        "df_train = one_hot_encode_categorical_cols(df_train, cat_cols + cat_cols_extra)\n"
      ],
      "metadata": {
        "id": "oqZs61xPw8oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_YQ-FUQD9EF",
        "outputId": "f2fd8595-6cfc-4802-fd5b-3eb04ea26f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (300794, 222)\n",
            "Test shape: (100265, 222)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "'''\n",
        "train_df = continous_vars.copy()\n",
        "# Normalize columns\n",
        "for col in continous:\n",
        "  train_df[col] = (train_df[col] - train_df[col].min()) / (train_df[col].max() - train_df[col].min())\n",
        "\n",
        "# Get NaN value counts age_approx : 2798\n",
        "#nan_counts = continous_vars.isna().sum()\n",
        "mean_age = train_df['age_approx'].mean()\n",
        "train_df['age_approx'].fillna(mean_age, inplace=True)\n",
        "'''\n",
        "# Split dataset\n",
        "train, test = train_test_split(df_train, test_size=0.25, random_state=42)\n",
        "\n",
        "print(f'Train shape: {train.shape}')\n",
        "print(f'Test shape: {test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky3HQl26FXdT",
        "outputId": "c42bb1e6-26c7-4bc1-d643-4cd84cab200c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train, Initial: target\n",
            "0    300491\n",
            "1       303\n",
            "Name: count, dtype: int64\n",
            "Test, Initial: target\n",
            "0    100175\n",
            "1        90\n",
            "Name: count, dtype: int64\n",
            "Train Final target\n",
            "0    6009\n",
            "1     303\n",
            "Name: count, dtype: int64\n",
            "Test Final target\n",
            "0    2003\n",
            "1      90\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print('Train, Initial:', train['target'].value_counts())\n",
        "print('Test, Initial:', test['target'].value_counts())\n",
        "\n",
        "def undersample_data(df, divisor = 50):\n",
        "  # Undersample class 0 by 1/150 times\n",
        "  cls_indices = [idx for idx, cls in enumerate(df['target'].values) if cls == 0]\n",
        "  cls_1 = [idx for idx, cls in enumerate(df['target'].values) if cls == 1]\n",
        "  to_keep = len(cls_indices)//divisor\n",
        "  np.random.shuffle(cls_indices)\n",
        "  indices_to_keep = cls_indices[:to_keep]\n",
        "  all_indices = indices_to_keep + cls_1\n",
        "\n",
        "  np.random.shuffle(all_indices)\n",
        "  df = df.iloc[all_indices]\n",
        "  return df\n",
        "\n",
        "train = undersample_data(train)\n",
        "test = undersample_data(test)\n",
        "\n",
        "print('Train Final', train['target'].value_counts())\n",
        "print('Test Final', test['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w8xiIaOFaud"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "image_dir = 'Kaggle-ISIC-Challenge/dataset/train-image/image'\n",
        "IMAGE_SIZE = 224\n",
        "vgg_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights = 'imagenet')\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2qGt5jKGF4p"
      },
      "outputs": [],
      "source": [
        "# Data Generator\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "  def __init__(self, image_ids, df, labels, batch_size=16, dtype='float32'):\n",
        "\n",
        "    self.image_ids = image_ids\n",
        "    self.df = df\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.IMAGE_SIZE = 224\n",
        "    self.image_dir = 'Kaggle-ISIC-Challenge/dataset/train-image/image'\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(len(self.df) / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    batch_df = self.df[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "\n",
        "    # Image features\n",
        "    batch_ids = self.image_ids[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "\n",
        "    batch_images = []\n",
        "    image_features = []\n",
        "    for id in batch_ids:\n",
        "      q = Image.open(f'{image_dir}/{id}.jpg')\n",
        "      q = np.array(q.resize((self.IMAGE_SIZE, self.IMAGE_SIZE)))\n",
        "      features = vgg_model(np.array(q).reshape(1, self.IMAGE_SIZE, self.IMAGE_SIZE, 3))\n",
        "      image_features.append(np.array(Flatten()(features)).reshape(7*7*512,))\n",
        "      batch_images.append(q)\n",
        "\n",
        "    length = len(batch_images)\n",
        "\n",
        "    #image_features = tf.reshape(image_features, (length, -1))\n",
        "\n",
        "    numerical_features = batch_df.to_numpy()\n",
        "\n",
        "    # Batch labels\n",
        "    batch_y = self.labels[index*self.batch_size : (index+1)*self.batch_size]\n",
        "\n",
        "    return [tf.convert_to_tensor(image_features, dtype), tf.convert_to_tensor(numerical_features, dtype)], tf.convert_to_tensor(batch_y, dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkARtIzN1lYc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAei1MjGx7P2"
      },
      "outputs": [],
      "source": [
        "dtype= 'float32'\n",
        "train_labels = train['target'].values\n",
        "train_image_ids = train['isic_id'].values\n",
        "train_input = train.drop(['target', 'isic_id'], axis = 1)\n",
        "\n",
        "train_data_gen = DataGenerator(train_image_ids, train_input, train_labels, 16)\n",
        "\n",
        "test_labels = test['target'].values\n",
        "test_image_ids = test['isic_id'].values\n",
        "test_input = test.drop(['target', 'isic_id'], axis = 1)\n",
        "\n",
        "val_data_gen = DataGenerator(test_image_ids, test_input, test_labels,  16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMgYYIH0ykUW"
      },
      "outputs": [],
      "source": [
        "x, y = val_data_gen.__getitem__(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BrMoXb8lvct1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJrPipIkzkI9"
      },
      "outputs": [],
      "source": [
        "from sys import stdlib_module_names\n",
        "# Setting up a bayesian neural network\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "\n",
        "# A normal distribution (mean = 0, sd = 1) as prior for kernel weights\n",
        "def prior(kernel_size: int, bias_size : int, dtype):\n",
        "  '''\n",
        "  To apply baye's theorem we start with a prior distribution which represents our prior beliefs about the weight distribution\n",
        "  Prior distribution is not trainable\n",
        "  '''\n",
        "  n = kernel_size + bias_size\n",
        "  return lambda t: tfd.Independent(tfd.Normal(loc = tf.zeros(n, dtype = dtype), scale = 1.), reinterpreted_batch_ndims = 1)\n",
        "\n",
        "def random_gaussian_initializer(shape, dtype):\n",
        "  '''\n",
        "  Initialize random normal distributions for out surrogate posterior\n",
        "\n",
        "  shape: number of (means + standard deviations)\n",
        "  '''\n",
        "  n = int(shape/2)\n",
        "  loc_norm = tf.random_normal_initializer(mean = 0., stddev = 1.)\n",
        "  loc = tf.Variable(initial_value = loc_norm(shape = (n,), dtype=dtype))\n",
        "\n",
        "  scale_norm = tf.random_normal_initializer(mean = 2., stddev = 1.)\n",
        "  scale = tf.Variable(initial_value = scale_norm(shape = (n,), dtype=dtype))\n",
        "\n",
        "  return tf.concat([loc, scale], 0)\n",
        "\n",
        "\n",
        "def posterior_mean_field(kernel_size: int, bias_size: int, dtype):\n",
        "  '''\n",
        "  Posterior distribution is a keras sequential model of two layers\n",
        "  1. Layer 1 is made of 2*n trainable variables (representing mean and standard deviation for every kernel)\n",
        "  2. Layer 2 is a tf probability distribution (normal distribution in this case) which is initalized using variables\n",
        "  from the first layer\n",
        "\n",
        "  This distribution represent our final beliefs for the weights of neural network\n",
        "  '''\n",
        "  n = kernel_size + bias_size\n",
        "  c = np.log(np.expm1(1.))\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tfp.layers.VariableLayer(2 * n, dtype = dtype,\n",
        "                    initializer = lambda shape, dtype: random_gaussian_initializer(shape, dtype), trainable = True),\n",
        "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "          tfd.Normal(loc = t[..., :n], scale = 1e-5 + tf.nn.softplus(c + t[..., n:])), reinterpreted_batch_ndims = 1))\n",
        "      ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5YmHRKDixwn",
        "outputId": "cb529dea-62aa-4484-a086-f3708a348101"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 25088])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "x[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRhYuFYxj84z"
      },
      "outputs": [],
      "source": [
        "train.shape[0]\n",
        "examples = int(train.shape[0])\n",
        "\n",
        "kl_divergence = (lambda q, p, _: tfd.kl_divergence(q, p) / tf.cast(tf.shape(q.sample())[0], tf.float32))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUFdNq0MmABs",
        "outputId": "0793d350-8f28-4db5-edd5-a6de19fe8b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(y)#.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MimkCOTDlhBG",
        "outputId": "a2c46735-caea-4cae-df7b-76654aee658f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qm3S6ij45AY",
        "outputId": "693f85c2-7761-42af-a4ed-a7491c81a8dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-8437ec24ef7a>:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  n = int(shape/2)\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "# Neural Network\n",
        "image_model_in = Input(x[0].shape)\n",
        "image_model = tfpl.DenseVariational(units = 3056, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(image_model_in)\n",
        "image_model = tfpl.DenseVariational(units = 512, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(image_model)\n",
        "image_model = tfpl.DenseVariational(units = 128, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(image_model)\n",
        "\n",
        "numerical_model_in = Input(x[1].shape)\n",
        "numerical_model = tfpl.DenseVariational(units = 128, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(numerical_model_in)\n",
        "numerical_model = tfpl.DenseVariational(units = 64, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(numerical_model)\n",
        "\n",
        "concat = concatenate([image_model, numerical_model])\n",
        "ensemble_model_in = tfpl.DenseVariational(units = 128, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(concat)\n",
        "ensemble_model_in = tfpl.DenseVariational(units = 32, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'gelu')(ensemble_model_in)\n",
        "ensemble_model_out = tfpl.DenseVariational(units = 1, make_posterior_fn=posterior_mean_field, make_prior_fn=prior, kl_weight=1/examples, activation = 'sigmoid')(ensemble_model_in)\n",
        "ensemble_model_out = tfpl.DistributionLambda(lambda t: tfd.Bernoulli(logits = t))(ensemble_model_out)\n",
        "\n",
        "ensemble_model = Model(inputs = [image_model_in, numerical_model_in], outputs = ensemble_model_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8j_4bu9kGGY",
        "outputId": "baada5a4-fc1a-416b-abc1-87395a67fbfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omv2OgJl6qlC",
        "outputId": "eb99200d-cca2-4658-9301-fac0b45cabe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 81s 506ms/step - loss: nan - binary_accuracy: 0.9504 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 75s 499ms/step - loss: nan - binary_accuracy: 0.9548 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 75s 499ms/step - loss: nan - binary_accuracy: 0.9567 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 75s 501ms/step - loss: nan - binary_accuracy: 0.9500 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 75s 499ms/step - loss: nan - binary_accuracy: 0.9486 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 75s 501ms/step - loss: nan - binary_accuracy: 0.9529 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 75s 500ms/step - loss: nan - binary_accuracy: 0.9538 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 75s 499ms/step - loss: nan - binary_accuracy: 0.9473 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 75s 500ms/step - loss: nan - binary_accuracy: 0.9517 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 75s 499ms/step - loss: nan - binary_accuracy: 0.9513 - val_loss: nan - val_binary_accuracy: 0.9570\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - ETA: 0s - loss: nan - binary_accuracy: 0.9500"
          ]
        }
      ],
      "source": [
        "def nll_loss(y_true, y_pred):\n",
        "  #dist = tfp.distributions.Bernoulli(logits = y_pred)\n",
        "  nll = -y_pred.log_prob(y_true)\n",
        "  #loss_kl = tf.keras.losses.KLD(y_true, y_pred)\n",
        "  #loss = tf.reduce_mean(nll + loss_kl)\n",
        "  return nll\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 15:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "ensemble_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002), loss = nll_loss, metrics = [tf.keras.metrics.BinaryAccuracy()])\n",
        "ensemble_model.fit(train_data_gen, validation_data = val_data_gen, epochs = 50, steps_per_epoch = 150)#, callbacks = [callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnKX1lIHlbWj"
      },
      "outputs": [],
      "source": [
        "# Save model weights\n",
        "ensemble_model.save_weights('/content/gdrive/MyDrive/Kaggle/ensemble_model_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwKtnfy23TB_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "acc452c8-076a-4cca-db2e-f47096e4486a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-ef797c2b3b5b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tvf2CF_3wgPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}